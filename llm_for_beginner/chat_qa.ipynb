{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分隔符\n",
    "delimiter = \"```\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \\\n",
    "    Respond in a friendly and helpful tone, with concise answers. \\\n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "product_information = f\"\"\"\n",
    "    Product: TechPro Ultra_book\n",
    "    Category: Computers and Laptops\n",
    "    Brand: TechPro\n",
    "    Model Number: TP-UB100\n",
    "    Warranty: 1 year\n",
    "    Rating: 4.5\n",
    "    Price: $799.99\n",
    "    Description: A sleek, 13.3-inch laptop with a 1920x1080 display, 8GB RAM, 256GB SSD storage, and Intel Core i5 processor.\n",
    "    \"\"\"\n",
    "user_input = \"你好,我想买一台电脑\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": f\"{delimiter}{user_input}{delimiter}\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Relevant product information:\\n{product_information}\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '```你好,我想买一台电脑```'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Relevant product information:\\n\\n    Product: TechPro Ultra_book\\n    Category: Computers and Laptops\\n    Brand: TechPro\\n    Model Number: TP-UB100\\n    Warranty: 1 year\\n    Rating: 4.5\\n    Price: $799.99\\n    Description: A sleek, 13.3-inch laptop with a 1920x1080 display, 8GB RAM, 256GB SSD storage, and Intel Core i5 processor.\\n    '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the langchain and langchain_community packages\n",
    "# %pip install langchain langchain_community\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# # 这里将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# # 如果想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "# chat = ChatOpenAI(temperature=0.0, openai_api_key=\"sk-xxx\")\n",
    "# chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/wyn_k9d108zfv2y674p1hv_r0000gn/T/ipykernel_82332/2376019639.py:10: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  chat = ChatOllama(model=\"huihui_ai/deepseek-r1-abliterated:7b\", temperature=0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问：你好，你是谁？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n我是DeepSeek-R1，一个由深度求索公司开发的智能助手，由350亿个参数训练而成。\\n</think>\\n\\n我是DeepSeek-R1，一个由深度求索公司开发的智能助手，由350亿个参数训练而成。', additional_kwargs={}, response_metadata={'model': 'huihui_ai/deepseek-r1-abliterated:7b', 'created_at': '2025-02-24T11:50:41.992916Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3509031625, 'load_duration': 560594458, 'prompt_eval_count': 18, 'prompt_eval_duration': 1818000000, 'eval_count': 59, 'eval_duration': 928000000}, id='run-bae50b01-add7-4750-a606-90c09572baad-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author: MingChen\n",
    "Date: 2025-01-25 19:51:00\n",
    "LastEditors: MingChen\n",
    "LastEditTime: 2025-02-18 13:21:06\n",
    "'''\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOllama(model=\"huihui_ai/deepseek-r1-abliterated:7b\", temperature=0.7)\n",
    "content = \"你好，你是谁？\"\n",
    "print(\"问：\" + content)\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个情感小助手，你叫PP。\"),\n",
    "    HumanMessage(content=content),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.chains import LLMChain   \n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'云舒Bedding'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"描述制造{product}的一个公司的最佳名称是什么? 只说一个名字。\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"大号床单套装\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mQuenessence\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mQuenessence specializes in natural skincare solutions, harnessing botanical extracts to create effective, gentle products for all skin types.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quenessence specializes in natural skincare solutions, harnessing botanical extracts to create effective, gentle products for all skin types.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? use only one word.\"\n",
    ")\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True\n",
    ")\n",
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI  # 导入OpenAI模型\n",
    "from langchain.prompts import ChatPromptTemplate  # 导入聊天提示模板\n",
    "from langchain.chains import LLMChain  # 导入LLM链。\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子链1\n",
    "# prompt模板 1: 翻译成英语（把下面的review翻译成英语）\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"把下面的评论review翻译成英文:\" \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: 输入：Review    输出：英文的 Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "# 子链2\n",
    "# prompt模板 2: 用一句话总结下面的 review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"请你用一句话来总结下面的评论review:\" \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: 输入：英文的Review   输出：总结\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "\n",
    "# 子链3\n",
    "# prompt模板 3: 下面review使用的什么语言\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"下面的评论review使用的什么语言:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: 输入：Review  输出：语言\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "\n",
    "# 子链4\n",
    "# prompt模板 4: 使用特定的语言对下面的总结写一个后续回复\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"使用特定的语言对下面的总结写一个后续回复:\"\n",
    "    \"\\n\\n总结: {summary}\\n\\n语言: {language}\"\n",
    ")\n",
    "# chain 4: 输入： 总结, 语言    输出： 后续回复\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：review\n",
    "# 输出：英文review，总结，后续回复\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't last, it's strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': '这款产品的口感一般，泡沫不易持久，且与商店购买的同款产品味道有明显差异，怀疑是旧批次或假货。',\n",
       " 'followup_message': \"Réponse suivante en français :\\n\\nCela semble être un avis négatif concernant un produit. Le goût est jugé moyen, la mousse ne persiste pas longtemps et il y a une différence notable par rapport à la version achetée dans un magasin local. L'acheteur soupçonne que ce pourrait être du stock vieilli ou même de fausse provenance. Il serait utile d'enquêter davantage sur l'origine et la qualité du produit pour confirmer ces préoccupations.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../data/Data.csv\")\n",
    "# review = df.Review[5]\n",
    "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain  # 导入多提示链\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文\n",
    "# 第一个提示适合回答物理问题\n",
    "physics_template = \"\"\"你是一个非常聪明的物理专家。 \\\n",
    "你擅长用一种简洁并且易于理解的方式去回答问题。\\\n",
    "当你不知道问题的答案时，你承认\\\n",
    "你不知道.\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第二个提示适合回答数学问题\n",
    "math_template = \"\"\"你是一个非常优秀的数学家。 \\\n",
    "你擅长回答数学问题。 \\\n",
    "你之所以如此优秀， \\\n",
    "是因为你能够将棘手的问题分解为组成部分，\\\n",
    "回答组成部分，然后将它们组合在一起，回答更广泛的问题。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第三个适合回答历史问题\n",
    "history_template = \"\"\"你是以为非常优秀的历史学家。 \\\n",
    "你对一系列历史时期的人物、事件和背景有着极好的学识和理解\\\n",
    "你有能力思考、反思、辩证、讨论和评估过去。\\\n",
    "你尊重历史证据，并有能力利用它来支持你的解释和判断。\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第四个适合回答计算机问题\n",
    "computerscience_template = \"\"\" 你是一个成功的计算机科学专家。\\\n",
    "你有创造力、协作精神、\\\n",
    "前瞻性思维、自信、解决问题的能力、\\\n",
    "对理论和算法的理解以及出色的沟通技巧。\\\n",
    "你非常擅长回答编程问题。\\\n",
    "你之所以如此优秀，是因为你知道  \\\n",
    "如何通过以机器可以轻松解释的命令式步骤描述解决方案来解决问题，\\\n",
    "并且你知道如何选择在时间复杂性和空间复杂性之间取得良好平衡的解决方案。\n",
    "\n",
    "这还是一个输入：\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"名字\": \"物理学\",\n",
    "        \"描述\": \"擅长回答关于物理学的问题\",\n",
    "        \"提示模板\": physics_template,\n",
    "    },\n",
    "    {\"名字\": \"数学\", \"描述\": \"擅长回答数学问题\", \"提示模板\": math_template},\n",
    "    {\"名字\": \"历史\", \"描述\": \"擅长回答历史问题\", \"提示模板\": history_template},\n",
    "    {\n",
    "        \"名字\": \"计算机科学\",\n",
    "        \"描述\": \"擅长回答计算机科学问题\",\n",
    "        \"提示模板\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "物理学: 擅长回答关于物理学的问题\n",
      "数学: 擅长回答数学问题\n",
      "历史: 擅长回答历史问题\n",
      "计算机科学: 擅长回答计算机科学问题\n"
     ]
    }
   ],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"名字\"]\n",
    "    prompt_template = p_info[\"提示模板\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['名字']}: {p['描述']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建默认目标链, 当路由器无法决定使用哪个子链时调用的链\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多提示路由模板\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"给语言模型一个原始文本输入，\\\n",
    "让其选择最适合输入的模型提示。\\\n",
    "系统将为您提供可用提示的名称以及最适合改提示的描述。\\\n",
    "如果你认为修改原始输入最终会导致语言模型做出更好的响应，\\\n",
    "你也可以修改原始输入。\n",
    "\n",
    "\n",
    "<< 格式 >>\n",
    "返回一个带有JSON对象的markdown代码片段，该JSON对象的格式如下：\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "\n",
    "\n",
    "\n",
    "记住：“destination”必须是下面指定的候选提示名称之一，\\\n",
    "或者如果输入不太适合任何候选提示，\\\n",
    "则可以是 “DEFAULT” 。\n",
    "记住：如果您认为不需要任何修改，\\\n",
    "则 “next_inputs” 可以只是原始输入。\n",
    "\n",
    "<< 候选提示 >>\n",
    "{destinations}\n",
    "\n",
    "<< 输入 >>\n",
    "{{input}}\n",
    "\n",
    "<< 输出 (记得要包含 ```json)>>\n",
    "\n",
    "样例:\n",
    "<< 输入 >>\n",
    "\"什么是黑体辐射?\"\n",
    "<< 输出 >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "# print(router_template)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiPromptChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 多提示链\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mMultiPromptChain\u001b[49m(\n\u001b[1;32m      3\u001b[0m     router_chain\u001b[38;5;241m=\u001b[39mrouter_chain,  \u001b[38;5;66;03m# 路由链路\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     destination_chains\u001b[38;5;241m=\u001b[39mdestination_chains,  \u001b[38;5;66;03m# 目标链路\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     default_chain\u001b[38;5;241m=\u001b[39mdefault_chain,  \u001b[38;5;66;03m# 默认链路\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiPromptChain' is not defined"
     ]
    }
   ],
   "source": [
    "# 多提示链\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,  # 路由链路\n",
    "    destination_chains=destination_chains,  # 目标链路\n",
    "    default_chain=default_chain,  # 默认链路\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m什么是黑体辐射？\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "chain.run(\"什么是黑体辐射？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2+2等于多少？\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "chain.run(\"2+2等于多少？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
