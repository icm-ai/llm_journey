{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分隔符\n",
    "delimiter = \"```\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \\\n",
    "    Respond in a friendly and helpful tone, with concise answers. \\\n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "product_information = f\"\"\"\n",
    "    Product: TechPro Ultra_book\n",
    "    Category: Computers and Laptops\n",
    "    Brand: TechPro\n",
    "    Model Number: TP-UB100\n",
    "    Warranty: 1 year\n",
    "    Rating: 4.5\n",
    "    Price: $799.99\n",
    "    Description: A sleek, 13.3-inch laptop with a 1920x1080 display, 8GB RAM, 256GB SSD storage, and Intel Core i5 processor.\n",
    "    \"\"\"\n",
    "user_input = \"你好,我想买一台电脑\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": f\"{delimiter}{user_input}{delimiter}\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Relevant product information:\\n{product_information}\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '```你好,我想买一台电脑```'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Relevant product information:\\n\\n    Product: TechPro Ultra_book\\n    Category: Computers and Laptops\\n    Brand: TechPro\\n    Model Number: TP-UB100\\n    Warranty: 1 year\\n    Rating: 4.5\\n    Price: $799.99\\n    Description: A sleek, 13.3-inch laptop with a 1920x1080 display, 8GB RAM, 256GB SSD storage, and Intel Core i5 processor.\\n    '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the langchain and langchain_community packages\n",
    "# %pip install langchain langchain_community\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# # 这里将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# # 如果想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "# chat = ChatOpenAI(temperature=0.0, openai_api_key=\"sk-xxx\")\n",
    "# chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/wyn_k9d108zfv2y674p1hv_r0000gn/T/ipykernel_63307/2376019639.py:10: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  chat = ChatOllama(model=\"huihui_ai/deepseek-r1-abliterated:7b\", temperature=0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问：你好，你是谁？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n我是DeepSeek-R1，一个由深度求索公司开发的智能助手，我会尽我所能为您提供帮助。\\n</think>\\n\\n我是DeepSeek-R1，一个由深度求索公司开发的智能助手，我会尽我所能为您提供帮助。', additional_kwargs={}, response_metadata={'model': 'huihui_ai/deepseek-r1-abliterated:7b', 'created_at': '2025-02-24T12:11:15.981618Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2060559625, 'load_duration': 566584000, 'prompt_eval_count': 18, 'prompt_eval_duration': 429000000, 'eval_count': 53, 'eval_duration': 856000000}, id='run-1369029f-e630-46b0-b552-50908f22c750-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author: MingChen\n",
    "Date: 2025-01-25 19:51:00\n",
    "LastEditors: MingChen\n",
    "LastEditTime: 2025-02-18 13:21:06\n",
    "'''\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOllama(model=\"huihui_ai/deepseek-r1-abliterated:7b\", temperature=0.7)\n",
    "content = \"你好，你是谁？\"\n",
    "print(\"问：\" + content)\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个情感小助手，你叫PP。\"),\n",
    "    HumanMessage(content=content),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.chains import LLMChain   \n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'云舒Bedding'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"描述制造{product}的一个公司的最佳名称是什么? 只说一个名字。\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"大号床单套装\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mQuenessence\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mQuenessence specializes in natural skincare solutions, harnessing botanical extracts to create effective, gentle products for all skin types.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quenessence specializes in natural skincare solutions, harnessing botanical extracts to create effective, gentle products for all skin types.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? use only one word.\"\n",
    ")\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True\n",
    ")\n",
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI  # 导入OpenAI模型\n",
    "from langchain.prompts import ChatPromptTemplate  # 导入聊天提示模板\n",
    "from langchain.chains import LLMChain  # 导入LLM链。\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子链1\n",
    "# prompt模板 1: 翻译成英语（把下面的review翻译成英语）\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"把下面的评论review翻译成英文:\" \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: 输入：Review    输出：英文的 Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "# 子链2\n",
    "# prompt模板 2: 用一句话总结下面的 review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"请你用一句话来总结下面的评论review:\" \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: 输入：英文的Review   输出：总结\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "\n",
    "# 子链3\n",
    "# prompt模板 3: 下面review使用的什么语言\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"下面的评论review使用的什么语言:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: 输入：Review  输出：语言\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "\n",
    "# 子链4\n",
    "# prompt模板 4: 使用特定的语言对下面的总结写一个后续回复\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"使用特定的语言对下面的总结写一个后续回复:\"\n",
    "    \"\\n\\n总结: {summary}\\n\\n语言: {language}\"\n",
    ")\n",
    "# chain 4: 输入： 总结, 语言    输出： 后续回复\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：review\n",
    "# 输出：英文review，总结，后续回复\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't last, it's strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': '这款产品的口感一般，泡沫不易持久，且与商店购买的同款产品味道有明显差异，怀疑是旧批次或假货。',\n",
       " 'followup_message': \"Réponse suivante en français :\\n\\nCela semble être un avis négatif concernant un produit. Le goût est jugé moyen, la mousse ne persiste pas longtemps et il y a une différence notable par rapport à la version achetée dans un magasin local. L'acheteur soupçonne que ce pourrait être du stock vieilli ou même de fausse provenance. Il serait utile d'enquêter davantage sur l'origine et la qualité du produit pour confirmer ces préoccupations.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../data/Data.csv\")\n",
    "# review = df.Review[5]\n",
    "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain  # 导入多提示链\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文\n",
    "# 第一个提示适合回答物理问题\n",
    "physics_template = \"\"\"你是一个非常聪明的物理专家。 \\\n",
    "你擅长用一种简洁并且易于理解的方式去回答问题。\\\n",
    "当你不知道问题的答案时，你承认\\\n",
    "你不知道.\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第二个提示适合回答数学问题\n",
    "math_template = \"\"\"你是一个非常优秀的数学家。 \\\n",
    "你擅长回答数学问题。 \\\n",
    "你之所以如此优秀， \\\n",
    "是因为你能够将棘手的问题分解为组成部分，\\\n",
    "回答组成部分，然后将它们组合在一起，回答更广泛的问题。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第三个适合回答历史问题\n",
    "history_template = \"\"\"你是以为非常优秀的历史学家。 \\\n",
    "你对一系列历史时期的人物、事件和背景有着极好的学识和理解\\\n",
    "你有能力思考、反思、辩证、讨论和评估过去。\\\n",
    "你尊重历史证据，并有能力利用它来支持你的解释和判断。\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "# 第四个适合回答计算机问题\n",
    "computerscience_template = \"\"\" 你是一个成功的计算机科学专家。\\\n",
    "你有创造力、协作精神、\\\n",
    "前瞻性思维、自信、解决问题的能力、\\\n",
    "对理论和算法的理解以及出色的沟通技巧。\\\n",
    "你非常擅长回答编程问题。\\\n",
    "你之所以如此优秀，是因为你知道  \\\n",
    "如何通过以机器可以轻松解释的命令式步骤描述解决方案来解决问题，\\\n",
    "并且你知道如何选择在时间复杂性和空间复杂性之间取得良好平衡的解决方案。\n",
    "\n",
    "这还是一个输入：\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"名字\": \"物理学\",\n",
    "        \"描述\": \"擅长回答关于物理学的问题\",\n",
    "        \"提示模板\": physics_template,\n",
    "    },\n",
    "    {\"名字\": \"数学\", \"描述\": \"擅长回答数学问题\", \"提示模板\": math_template},\n",
    "    {\"名字\": \"历史\", \"描述\": \"擅长回答历史问题\", \"提示模板\": history_template},\n",
    "    {\n",
    "        \"名字\": \"计算机科学\",\n",
    "        \"描述\": \"擅长回答计算机科学问题\",\n",
    "        \"提示模板\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "物理学: 擅长回答关于物理学的问题\n",
      "数学: 擅长回答数学问题\n",
      "历史: 擅长回答历史问题\n",
      "计算机科学: 擅长回答计算机科学问题\n"
     ]
    }
   ],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"名字\"]\n",
    "    prompt_template = p_info[\"提示模板\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['名字']}: {p['描述']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建默认目标链, 当路由器无法决定使用哪个子链时调用的链\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多提示路由模板\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"给语言模型一个原始文本输入，\\\n",
    "让其选择最适合输入的模型提示。\\\n",
    "系统将为您提供可用提示的名称以及最适合改提示的描述。\\\n",
    "如果你认为修改原始输入最终会导致语言模型做出更好的响应，\\\n",
    "你也可以修改原始输入。\n",
    "\n",
    "\n",
    "<< 格式 >>\n",
    "返回一个带有JSON对象的markdown代码片段，该JSON对象的格式如下：\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "\n",
    "\n",
    "\n",
    "记住：“destination”必须是下面指定的候选提示名称之一，\\\n",
    "或者如果输入不太适合任何候选提示，\\\n",
    "则可以是 “DEFAULT” 。\n",
    "记住：如果您认为不需要任何修改，\\\n",
    "则 “next_inputs” 可以只是原始输入。\n",
    "\n",
    "<< 候选提示 >>\n",
    "{destinations}\n",
    "\n",
    "<< 输入 >>\n",
    "{{input}}\n",
    "\n",
    "<< 输出 (记得要包含 ```json)>>\n",
    "\n",
    "样例:\n",
    "<< 输入 >>\n",
    "\"什么是黑体辐射?\"\n",
    "<< 输出 >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "# print(router_template)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多提示链\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,  # 路由链路\n",
    "    destination_chains=destination_chains,  # 目标链路\n",
    "    default_chain=default_chain,  # 默认链路\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "物理学: {'input': '什么是黑体辐射？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'黑体辐射是指理想化的物体（称为黑体）在温度升高时会发射出的所有类型电磁波的现象。简单来说，当一个物体被加热到足够高的温度时，它不仅会变热并发出热量，还会开始发光。黑体能够完全吸收所有入射的电磁辐射，而不反射任何光线，因此它的表面看起来是黑色的（尽管在高温下可以发出强烈的光）。\\n\\n黑体辐射的研究揭示了能量量子化和普朗克常数的概念，这是量子力学的基础之一。黑体辐射的谱线可以用一个公式来描述，这个公式是由马克斯·普朗克提出的，并且与温度有关。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"什么是黑体辐射？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "数学: {'input': '2+2等于多少？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2+2等于4。这个问题是一个基本的算术运算，不需要进一步分解即可直接得出答案。如果你有更复杂的数学问题需要帮助，欢迎随时提问！'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"2+2等于多少？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': '今天星期幾？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'很抱歉，我無法獲取當前的日期信息。您可以查看您的設備來得知今天是星期幾。如果您需要了解某個特定日期是星期幾，或者有其他問題需要幫助，請告訴我！'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"今天星期幾？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': '什麼是DNA？這應該是什麼類型的問題？你能回答嗎？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DNA，全稱為脫氧核糖核酸（Deoxyribonucleic Acid），是一種大分子化合物，對生物體遺傳信息的儲存和傳遞起著核心作用。它由數以千計至數百萬個核苷酸單元組成，這些單元按照特定序列排列，形成了基因和其他功能片段。\\n\\n這類問題屬於生命科學或生物學領域的基本知識問題。DNA是理解遺傳、進化以及分子生物學等多方面的重要概念。\\n\\n當然，我可以回答這個問題。你有沒有具體的子問題或者想要更深入地了解某一方面呢？'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里，我们问了一个关于生物学的问题，我们可以看到它选择的链路是无。这意味着它将被传递到默认链路，它本身只是对语言模型的通用调用。语言模型幸运地对生物学知道很多，所以它可以帮助我们。\n",
    "chain.run(\"什麼是DNA？這應該是什麼類型的問題？你能回答嗎？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
